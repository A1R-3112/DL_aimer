{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import platform\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':2048,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "PATH = os.getcwd() + '/data/'\n",
    "if platform.system() == 'Darwin':\n",
    "    LOADPATH = '/Users/a1r/Desktop/DL/timeseries_new_data/'\n",
    "else:\n",
    "    LOADPATH = '/home/a1r/바탕화면/DL/timeseries_new_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "new_train = pd.read_csv(LOADPATH + 'train_fe.csv', low_memory=False)\n",
    "new_train = new_train.sort_values(by = ['ID', 'date']).reset_index(drop = True)\n",
    "origin_train = pd.read_csv(PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Features\n",
    "#### New_Train : train_data after feature engineering\n",
    "O : 학습에 사용될 Features\n",
    "X : 학습에서 drop할 Features\n",
    "\n",
    "* [O] sales - 제품의 일별 판매량  => **Target**\n",
    "* [X] ID - 제품 ID\n",
    "* [X] 제품 - 제품 코드\n",
    "* [O] 대분류 - 제품의 대분류\n",
    "* [O] 중분류 - 제품의 중분류\n",
    "* [O] 소분류 - 제품의 소분류\n",
    "* [O] 브랜드 - 제품의 브랜드\n",
    "* [X] date - 제품의 판매 날짜\n",
    "    *  `23.02.23 ~ 23.03.28` : 약 92.65%의 상품이 이 기간동안 0임을 알 수 있음\n",
    "* [O] quarter - 제품의 판매 분기 (1, 2, 3, 4)분기 존재\n",
    "* [O] day_name - 제품의 판매 요일\n",
    "* [O] keyword - 정규화된 제품 브랜드의 키워드 언급 횟수 : 브랜드의 인지도로 판단\n",
    "* [O] price - 제품의 판매 가격(₩)\n",
    "* [O] event - 해당 날짜에 event가 있음 (binary? or Category?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_FE(df):\n",
    "    one = df.query('event != \"0\"')\n",
    "    one.event = np.ones(len(one), dtype = np.int16)\n",
    "    df.loc[one.index, 'event'] = one.event\n",
    "    df.event = df.event.astype(np.int16)\n",
    "\n",
    "    drop_date = []\n",
    "    df_enc = df.copy()\n",
    "    columns = ['ID', '대분류', '중분류', '소분류', '브랜드', 'day_name', 'quarter', 'keyword', 'event', 'sales', 'date']\n",
    "\n",
    "    for i in range(34):\n",
    "        drop_date.append(list(np.array(pd.date_range('2023-02-23', periods = 34)).astype(str))[i].split('T')[0])\n",
    "\n",
    "    for date in tqdm(drop_date):\n",
    "        drop_idx = df_enc.query('date == @date').index\n",
    "        df_enc.drop(drop_idx, axis = 0, inplace = True)\n",
    "    \n",
    "    # 요일 명에서 주중 주말로 변경 : Category -> Binary\n",
    "    df_enc['day_name'] = df_enc['day_name'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "    df_enc.drop(['제품','year', 'month', 'day', 'day_of_week', 'price'], axis = 1, inplace = True)\n",
    "    df_enc = df_enc[columns]\n",
    "    df_enc = df_enc.drop_duplicates()\n",
    "    df_enc = df_enc.reset_index(drop = True)\n",
    "\n",
    "    return df_enc\n",
    "\n",
    "train_enc = data_FE(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "\n",
    "col = ['대분류', '중분류', '소분류', '브랜드', 'day_name']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "for c in col:\n",
    "    train_enc[c] = encoder.fit_transform(train_enc[c])\n",
    "\n",
    "train_enc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler about Each ID\n",
    "def MinMaxscaler(df):\n",
    "    min_arr = []\n",
    "    max_arr = []\n",
    "    ID_num = df.ID.nunique()\n",
    "\n",
    "    for i in tqdm(range(ID_num)):\n",
    "        date_cluster = df.iloc[i*425:(i+1)*425]\n",
    "        min_ = date_cluster.sales.min(axis = 0)\n",
    "        max_ = date_cluster.sales.max(axis = 0)\n",
    "        denom = max_ - min_\n",
    "\n",
    "        if min_ == max_:\n",
    "            df.iloc[i*425:(i+1)*425, 9] = 0  # NaN값 방지\n",
    "        else:\n",
    "            df.iloc[i*425:(i+1)*425, 9] = (date_cluster.sales - min_) / denom\n",
    "\n",
    "        min_arr.append(min_)\n",
    "        max_arr.append(max_)\n",
    "\n",
    "    min_arr = np.array(min_arr)\n",
    "    max_arr = np.array(max_arr)\n",
    "\n",
    "    return df, min_arr, max_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc, _, _ = MinMaxscaler(train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train & Test Dataset to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  시간이 너무 오래걸림\n",
    "## 현실적으로 쓸 수 없는 함수\n",
    "## pandas 라이브러리가 너무 무거운 거로 판단됨 -> numpy 데이터로 변형 후 함수 적용하는게 맞는듯\n",
    "## numpy로 바꿔도 차이가 없음 : 시간복잡도가 너무 높아서 생기는 문제로 판단됨 O(n^2)\n",
    "## pandas.DataFrame.query()의 문제라고 판명\n",
    "## ID및 date의 순서로 되어있기 때문에 iloc을 사용해 순서대로 잘라서 dataset을 만들어서 작업 소요시간이 매우 줄음\n",
    "\n",
    "def make_train_data(data, train_size = CFG['TRAIN_WINDOW_SIZE'], predict_size = CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : date를 melt시킨 새로운 train data\n",
    "    train_size : 학습에 활용할 기간 => 90 Days\n",
    "    predict_size : 추론할 기간 => 21 Days\n",
    "    '''\n",
    "    window_size = train_size + predict_size         # 90 + 21 = 111\n",
    "    num_id = data.ID.nunique()                      # ID: 15890\n",
    "    num_date = data.date.nunique()                  # 날짜: 425\n",
    "    num_features = len(data.iloc[0, 1:9])           # date를 제외한 나머지 features : 대분류 ~ sales / sales <- Target\n",
    "    data = np.array(data)                           # DataFrame to Numpy Data\n",
    "    \n",
    "    input_data = np.empty((num_id * ((num_date + num_features) - window_size + 1), train_size, num_features + 1), dtype = np.float16)\n",
    "    target_data = np.empty((num_id * ((num_date + num_features) - window_size + 1), predict_size), dtype = np.float16)\n",
    "\n",
    "    for id in tqdm(range(num_id)):\n",
    "        for j in range(num_date - window_size + 1):      # 315\n",
    "            temp_data = data[id*425: 425*(id+1)][j:train_size+j, 1:10]\n",
    "            input_data[id * ((num_date + num_features) - window_size + 1) + j] = temp_data\n",
    "            target_data[id * ((num_date + num_features) - window_size + 1) + j] = data[id*425: 425*(id+1)][train_size+j:window_size+j, 9] # sales\n",
    "\n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']): #90\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : date를 melt시킨 새로운 train data\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_id = data.ID.nunique()                      # ID: 15890\n",
    "    num_date = data.date.nunique()                  # 날짜: 425\n",
    "    num_features = len(data.iloc[0, 1:9])           # date를 제외한 나머지 features : 대분류 ~ sales / sales <- Target\n",
    "    data = np.array(data)\n",
    "    \n",
    "    test_input = np.empty((num_id, train_size, num_features + 1), dtype = np.float16)\n",
    "\n",
    "    for id in tqdm(range(num_id)):\n",
    "        temp_data = data[id*425: 425*(id+1)][-train_size:, 1:10]\n",
    "        test_input[id] = temp_data\n",
    "\n",
    "    return test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = make_train_data(train_enc)\n",
    "test_input = make_predict_data(train_enc)\n",
    "\n",
    "np.save(f'train_input_weeke', train_input)\n",
    "np.save(f'train_target_week', train_target)\n",
    "np.save(f'test_input_week', test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 대분류 별로 MinMaxScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_0 = train_enc.query('대분류 == 0')\n",
    "train_enc_1 = train_enc.query('대분류 == 1')\n",
    "train_enc_2 = train_enc.query('대분류 == 2')\n",
    "train_enc_3 = train_enc.query('대분류 == 3')\n",
    "train_enc_4 = train_enc.query('대분류 == 4')\n",
    "\n",
    "train_enc_large = [train_enc_0, train_enc_1, train_enc_2, train_enc_3, train_enc_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_0, min_0, max_0 = MinMaxscaler(train_enc_0)\n",
    "train_enc_1, min_1, max_1 = MinMaxscaler(train_enc_1)\n",
    "train_enc_2, min_2, max_2 = MinMaxscaler(train_enc_2)\n",
    "train_enc_3, min_3, max_3 = MinMaxscaler(train_enc_3)\n",
    "train_enc_4, min_4, max_4 = MinMaxscaler(train_enc_4)\n",
    "\n",
    "# np.save('MIN_0', min_0)\n",
    "# np.save('MAX_0', max_0)\n",
    "\n",
    "# np.save('MIN_1', min_1)\n",
    "# np.save('MAX_1', max_1)\n",
    "\n",
    "# np.save('MIN_2', min_2)\n",
    "# np.save('MAX_2', max_2)\n",
    "\n",
    "# np.save('MIN_3', min_3)\n",
    "# np.save('MAX_3', max_3)\n",
    "\n",
    "# np.save('MIN_4', min_4)\n",
    "# np.save('MAX_4', max_4)\n",
    "\n",
    "train_enc_large = [train_enc_0, train_enc_1, train_enc_2, train_enc_3, train_enc_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in tqdm(train_enc_large):\n",
    "    train_input, train_target = make_train_data(data)\n",
    "    test_input = make_predict_data(data)\n",
    "    \n",
    "    np.save(f'train_input_{i}', train_input)\n",
    "    np.save(f'train_target_{i}', train_target)\n",
    "    np.save(f'test_input_{i}', test_input)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대분류: 0\n",
    "    - train_input.shape: (1217064, 90, 9)\n",
    "    - train_target.shape: (1217064, 21)\n",
    "- 대분류: 1\n",
    "    - train_input.shape: (3538142, 90, 9)\n",
    "    - train_target.shape: (3538142, 21)\n",
    "- 대분류: 2\n",
    "    - train_input.shape: (143412, 90, 9)\n",
    "    - train_target.shape: (143412, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = make_train_data(train_enc)\n",
    "test_input = make_predict_data(train_enc)\n",
    "\n",
    "# np.save('train_input', train_input)\n",
    "# np.save('train_target', train_target)\n",
    "# np.save('test_input', test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA by Large Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID_num = 3768\n",
    "\n",
    "train_enc_0 = pd.read_csv(os.getcwd() + '/train_data_0.csv')\n",
    "train_enc_0 = train_enc_0.sort_values(by = 'date').reset_index()\n",
    "train_enc_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = train_enc_0.copy()\n",
    "\n",
    "for i in range(425):\n",
    "    key_mean = np.mean(train_enc_0.iloc[i*3768: (i+1) * 3768].keyword)\n",
    "    sales_sum = sum(train_enc_0.iloc[i*3768: (i+1) * 3768].sales)\n",
    "    train_0.iloc[i*3768:(i+1) * 3768, 10] = np.repeat(sales_sum, 3768)\n",
    "    train_0.iloc[i*3768:(i+1) * 3768, 8] = np.repeat(key_mean, 3768)\n",
    "\n",
    "drop_col = ['index', 'ID', '중분류', '소분류', '브랜드']\n",
    "train_0 = train_0.drop(drop_col, axis = 1).drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_1 = pd.read_csv(PATH + 'GRU_1e_4_drop(0.4).csv')\n",
    "lstm_1 = pd.read_csv(PATH + 'lstm_first.csv')\n",
    "gru_2 = pd.read_csv(PATH + 'GRU_2_layers_1e_4.csv')\n",
    "lstm_2 = pd.read_csv(PATH + 'lstm_2Layers_0820.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_all = lstm_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = gru_1.iloc[:, 1:] + gru_2.iloc[:, 1:] + lstm_1.iloc[:, 1:] + lstm_2.iloc[:, 1:]\n",
    "blend_all.iloc[:, 1:] = np.around(blend/4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_all.to_csv(PATH + 'Blend_0820.csv', index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
