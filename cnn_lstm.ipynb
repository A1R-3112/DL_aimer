{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import platform\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':5e-6,\n",
    "    'BATCH_SIZE':2048,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "PATH = os.getcwd() + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>제품</th>\n",
       "      <th>대분류</th>\n",
       "      <th>중분류</th>\n",
       "      <th>소분류</th>\n",
       "      <th>브랜드</th>\n",
       "      <th>2022-01-01</th>\n",
       "      <th>2022-01-02</th>\n",
       "      <th>2022-01-03</th>\n",
       "      <th>2022-01-04</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-03-26</th>\n",
       "      <th>2023-03-27</th>\n",
       "      <th>2023-03-28</th>\n",
       "      <th>2023-03-29</th>\n",
       "      <th>2023-03-30</th>\n",
       "      <th>2023-03-31</th>\n",
       "      <th>2023-04-01</th>\n",
       "      <th>2023-04-02</th>\n",
       "      <th>2023-04-03</th>\n",
       "      <th>2023-04-04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-00001-00001</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B002-00002-00001</td>\n",
       "      <td>B002-C001-0003</td>\n",
       "      <td>B002-C002-0008</td>\n",
       "      <td>B002-C003-0044</td>\n",
       "      <td>B002-00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>B002-00002-00002</td>\n",
       "      <td>B002-C001-0003</td>\n",
       "      <td>B002-C002-0008</td>\n",
       "      <td>B002-C003-0044</td>\n",
       "      <td>B002-00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                제품             대분류             중분류             소분류  \\\n",
       "0   0  B002-00001-00001  B002-C001-0002  B002-C002-0007  B002-C003-0038   \n",
       "1   1  B002-00002-00001  B002-C001-0003  B002-C002-0008  B002-C003-0044   \n",
       "2   2  B002-00002-00002  B002-C001-0003  B002-C002-0008  B002-C003-0044   \n",
       "\n",
       "          브랜드  2022-01-01  2022-01-02  2022-01-03  2022-01-04  ...  \\\n",
       "0  B002-00001           0           0           0           0  ...   \n",
       "1  B002-00002           0           0           0           0  ...   \n",
       "2  B002-00002           0           0           0           0  ...   \n",
       "\n",
       "   2023-03-26  2023-03-27  2023-03-28  2023-03-29  2023-03-30  2023-03-31  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           1           3           2   \n",
       "2           0           0           0           0           0           0   \n",
       "\n",
       "   2023-04-01  2023-04-02  2023-04-03  2023-04-04  \n",
       "0           0           0           0           0  \n",
       "1           0           0           2           0  \n",
       "2           0           0           0           0  \n",
       "\n",
       "[3 rows x 465 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(PATH + 'train.csv')\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM MODEL\n",
    "![스크린샷 2023-08-04 오후 5 16 00](https://github.com/Megvii-BaseDetection/YOLOX/assets/103639510/6e612140-39b5-4198-bb14-81ff2aca5f22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_LSTM(nn.Module):\n",
    "    def __init__(self, in_channel=5, hidden_size = 64, out_channel=CFG['PREDICT_SIZE']):\n",
    "        super(Conv1d_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=in_channel,\n",
    "                                out_channels=32,\n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1)\n",
    "        \n",
    "        # self.conv1d_2 = nn.Conv1d(in_channels=16,\n",
    "        #                         out_channels=32,\n",
    "        #                         kernel_size=3,\n",
    "        #                         stride=1,\n",
    "        #                         padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = 32,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=1,\n",
    "                            bias=True,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, out_channel)\n",
    "        )\n",
    "\n",
    "        self.actv = nn.ReLU()\n",
    "        # self.dense1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        # self.dropout = nn.Dropout()\n",
    "        # self.dense2 = nn.Linear(hidden_size//2, out_channel)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (\n",
    "            torch.zeros(1, batch_size, self.hidden_size, device = device),\n",
    "            torch.zeros(1, batch_size, self.hidden_size, device = device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\t# Raw x shape : (B, TRAIN_WINDOW_SIZE, in_channel) => (B, 90, 5)\n",
    "        \n",
    "        # Shape : (B, F: in_channel, S: TRAIN_WINDOW_SIZE) => (B, 5, 90)\n",
    "        x = x.transpose(1, 2)\n",
    "        # Shape with Conv1d_1 : (B, F, S) == (B, C, S) || C = channel => (B, 16, 90)\n",
    "        x = self.conv1d_1(x)\n",
    "        ## Shape with Conv1d_2 : (B, C, S) => (B, 32, 90)\n",
    "        # x = self.conv1d_2(x)\n",
    "        ## Reshape : (B, S, C) == (B, S, F) => (B, 90, 32)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # self.lstm.flatten_parameters()\n",
    "        # # Shape : (B, S, H) // H = hidden_size => (B, 90, 512)\n",
    "        # _, (hidden, _) = self.lstm(x)\n",
    "        # # Shape : (B, H) // -1 means the last sequence => (B, 512)\n",
    "        # x = hidden[-1]\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        # LSTM Layer\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # Only use the last output sequence\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "\n",
    "        # Fully connected layer\n",
    "        output = self.actv(self.fc(last_output))\n",
    "        x = output.squeeze(1)\n",
    "        \n",
    "        # # Shape : (B, 512)\n",
    "        # x = self.dense1(x)\n",
    "        # # ReLU\n",
    "        # x = F.relu(x)\n",
    "        # # Shape : (B, H) => (B, 256)\n",
    "        # x = self.dropout(x)\n",
    "        # # Shape : (B, O) // O = output => (B, 21)\n",
    "        # x = self.dense2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns = ['ID', '제품'])\n",
    "\n",
    "# Data Scaling\n",
    "scale_max_dict = {}\n",
    "scale_min_dict = {}\n",
    "\n",
    "for idx in tqdm(range(len(train_data))):\n",
    "    maxi = np.max(train_data.iloc[idx,4:])\n",
    "    mini = np.min(train_data.iloc[idx,4:])\n",
    "    \n",
    "    if maxi == mini :\n",
    "        train_data.iloc[idx,4:] = 0\n",
    "    else:\n",
    "        train_data.iloc[idx,4:] = (train_data.iloc[idx,4:] - mini) / (maxi - mini)\n",
    "    \n",
    "    scale_max_dict[idx] = maxi\n",
    "    scale_min_dict[idx] = mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train_data[col])\n",
    "    train_data[col] = label_encoder.transform(train_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    '''\n",
    "    학습 기간 블럭, 예측 기간 블럭의 세트로 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 학습에 활용할 기간 => 90 Days\n",
    "    predict_size : 추론할 기간 => 21 Days\n",
    "    '''\n",
    "    num_rows = len(data) # 15890\n",
    "    window_size = train_size + predict_size # 90 + 21 = 111\n",
    "    \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, len(data.iloc[0, :4]) + 1), dtype = np.float16)\n",
    "    # (5609170, 90, 5)\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size), dtype = np.float16)\n",
    "    # (5640950, 21)\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])  # Label: 대분류, 중분류, 소분류, 브랜드\n",
    "        sales_data = np.array(data.iloc[i, 4:]) # 날짜 데이터: 2022-01-01 ~ 2023-04-04\n",
    "        \n",
    "        for j in range(len(sales_data) - window_size + 1): # 0 ~ 348\n",
    "            window = sales_data[j : j + window_size]\n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "            input_data[i * (len(data.columns) - window_size + 1) + j] = temp_data\n",
    "            target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n",
    "    \n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "    '''\n",
    "    평가 데이터(Test Dataset)를 추론하기 위한 Input 데이터를 생성\n",
    "    data : 일별 판매량\n",
    "    train_size : 추론을 위해 필요한 일별 판매량 기간 (= 학습에 활용할 기간)\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    \n",
    "    input_data = np.empty((num_rows, train_size, len(data.iloc[0, :4]) + 1), dtype = np.float16)\n",
    "    \n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "        \n",
    "        window = sales_data[-train_size : ]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size]))\n",
    "        input_data[i] = temp_data\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = make_train_data(train_data)\n",
    "test_input = make_predict_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4487336, 90, 5)\n",
      "(4487336, 21)\n",
      "(1121834, 90, 5)\n",
      "(1121834, 21)\n",
      "(15890, 90, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train / Validation Split\n",
    "data_len = len(train_input)\n",
    "val_input = train_input[-int(data_len*0.2):]\n",
    "val_target = train_target[-int(data_len*0.2):]\n",
    "train_input = train_input[:-int(data_len*0.2)]\n",
    "train_target = train_target[:-int(data_len*0.2)]\n",
    "\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(val_input.shape)\n",
    "print(val_target.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = CustomDataset(train_input, train_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_input, val_target)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_sfa(pred, df):\n",
    "    pred_length = pred.shape[1] - 1\n",
    "    true = df.iloc[:, -pred_length:].reset_index() \\\n",
    "        .rename(columns={\"index\": \"ID\"})\n",
    "\n",
    "    main_id = {}\n",
    "    for main_cat in df[\"대분류\"].unique():\n",
    "        main_id[main_cat] = df.query(\"대분류==@main_cat\")[\"ID\"].to_list()\n",
    "\n",
    "    psfa = []\n",
    "    for main_cat in main_id.keys():\n",
    "        indices = true[\"ID\"].isin(main_id[main_cat])\n",
    "\n",
    "        true_arr = true[indices].iloc[:, 1:].to_numpy()\n",
    "        pred_arr = pred[indices].iloc[:, 1:].to_numpy()\n",
    "\n",
    "        eps = np.ones((true_arr.shape)) / 1e8\n",
    "\n",
    "        true_sum = true_arr.sum(axis=0)\n",
    "        true_sum = np.stack([true_sum]*len(true_arr)) + eps\n",
    "        true_rate = true_arr / true_sum\n",
    "\n",
    "        abs_error = np.abs(true_arr - pred_arr)\n",
    "        denom = np.maximum(true_arr, pred_arr+eps)\n",
    "        \n",
    "        score = 1 - (1 / true_arr.shape[1]\n",
    "                     * (abs_error / denom) * true_rate).sum()\n",
    "        psfa.append(score)\n",
    "        print(main_cat, score)\n",
    "\n",
    "    return np.mean(psfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualMinMaxScaler:\n",
    "    def __get_min_val(self, df, date_cols):\n",
    "        df_mins = df[date_cols].min(axis=1).to_numpy()\n",
    "        return df_mins\n",
    "\n",
    "    def __get_max_val(self, df, date_cols):\n",
    "        df_maxs = df[date_cols].max(axis=1).to_numpy()\n",
    "        return df_maxs\n",
    "\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        date_cols = [col for col in df.columns  # 일자 변수들만 Scale하기\n",
    "                     if col.startswith(\"2\")]\n",
    "\n",
    "        self.min_val = self.__get_min_val(df, date_cols)\n",
    "        self.max_val = self.__get_max_val(df, date_cols)\n",
    "        denom = self.max_val - self.min_val\n",
    "        self.denom = np.where(denom==0, 1, denom)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame):\n",
    "        date_cols = [col for col in df.columns\n",
    "                     if col.startswith(\"2\")]\n",
    "        return df[date_cols] \\\n",
    "            .apply(lambda x: (x-self.min_val)/self.denom)\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame):\n",
    "        self.fit(df)\n",
    "        return self.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PsfaLoss(nn.Module):\n",
    "    def __init__(self, scaler, df):\n",
    "        super().__init__()\n",
    "        self.scaler = scaler\n",
    "        self.main_cats = df.groupby(\"대분류\")[\"ID\"].unique().values\n",
    "    \n",
    "    def forward(self, pred, true):\n",
    "        # pred: [batch_size, length, products(15890)]\n",
    "        pred = pred * torch.tensor(self.scaler.denom) \\\n",
    "            + torch.tensor(self.scaler.min_val)\n",
    "        true = true * torch.tensor(self.scaler.denom) \\\n",
    "            + torch.tensor(self.scaler.min_val)\n",
    "\n",
    "        L1scaled = torch.abs(true-pred) / torch.maximum(pred, true+1e-8)\n",
    "        \n",
    "        rate = torch.zeros_like(true)\n",
    "        for i in range(len(self.main_cats)):\n",
    "            rate[:, :, self.main_cats[i]] = \\\n",
    "                true[:, :, self.main_cats[i]] \\\n",
    "                / (true[:, :, self.main_cats[i]].sum(dim=-1, keepdim=True) + 1e-8) \\\n",
    "                / len(self.main_cats)\n",
    "        return (L1scaled * rate).sum() / (true.shape[0] * true.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_loss = 9999999\n",
    "    best_model = None\n",
    "    \n",
    "    # Epoch\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_mae = []\n",
    "    \n",
    "    # Iteration\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, val_loss_li = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            print('Model Saved')\n",
    "    \n",
    "    return best_model, train_loss, val_loss_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss), val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d_LSTM()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG['LEARNING_RATE'])\n",
    "infer_model, train_loss, val_loss = train(model, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.getcwd() + '/cnn_lstm.pth'\n",
    "torch.save(infer_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1d_1.weight',\n",
       "              tensor([[[-0.2139,  0.0341,  0.0178],\n",
       "                       [ 0.2430,  0.1748, -0.1341],\n",
       "                       [ 0.1950, -0.2251, -0.1796],\n",
       "                       [ 0.1951,  0.1846, -0.2366],\n",
       "                       [-0.2379,  0.2016, -0.1399]],\n",
       "              \n",
       "                      [[ 0.1218,  0.2366,  0.0295],\n",
       "                       [-0.2214,  0.1076, -0.1064],\n",
       "                       [-0.1019, -0.0861, -0.1100],\n",
       "                       [ 0.2166,  0.2255,  0.0938],\n",
       "                       [-0.2652,  0.0379,  0.0283]],\n",
       "              \n",
       "                      [[ 0.1974,  0.0710, -0.1311],\n",
       "                       [ 0.1524,  0.2426,  0.2417],\n",
       "                       [ 0.0776, -0.0773,  0.0488],\n",
       "                       [ 0.2244, -0.1414,  0.1487],\n",
       "                       [-0.0419,  0.1354, -0.0563]],\n",
       "              \n",
       "                      [[-0.0784, -0.0625,  0.1732],\n",
       "                       [-0.0773, -0.2054, -0.2235],\n",
       "                       [ 0.2032,  0.2080,  0.1791],\n",
       "                       [ 0.0180,  0.0456, -0.2547],\n",
       "                       [-0.2494, -0.2692, -0.0475]],\n",
       "              \n",
       "                      [[ 0.1528,  0.0220,  0.1746],\n",
       "                       [-0.0509,  0.0436, -0.1704],\n",
       "                       [ 0.1355, -0.2627, -0.0874],\n",
       "                       [-0.2081,  0.0052,  0.0080],\n",
       "                       [-0.0899,  0.2256,  0.0350]],\n",
       "              \n",
       "                      [[-0.0867,  0.1657,  0.2211],\n",
       "                       [-0.2259,  0.2285,  0.0176],\n",
       "                       [-0.0767, -0.1590, -0.1278],\n",
       "                       [ 0.1979,  0.0033, -0.1778],\n",
       "                       [ 0.2033,  0.1029, -0.1974]],\n",
       "              \n",
       "                      [[-0.2164, -0.2071, -0.1383],\n",
       "                       [ 0.0159,  0.0265,  0.1545],\n",
       "                       [-0.0196,  0.0573, -0.1756],\n",
       "                       [ 0.1627, -0.0649, -0.0313],\n",
       "                       [-0.2918,  0.1314, -0.0925]],\n",
       "              \n",
       "                      [[ 0.0190, -0.0105, -0.0445],\n",
       "                       [ 0.0264,  0.2474,  0.1793],\n",
       "                       [ 0.2164, -0.1286, -0.1070],\n",
       "                       [-0.1040, -0.1911, -0.0839],\n",
       "                       [-0.3010,  0.0441,  0.0234]],\n",
       "              \n",
       "                      [[-0.1561,  0.0702,  0.1714],\n",
       "                       [ 0.2018, -0.0142,  0.2370],\n",
       "                       [ 0.0143,  0.1383,  0.0827],\n",
       "                       [ 0.1126,  0.1935,  0.0166],\n",
       "                       [-0.1316,  0.0889, -0.2288]],\n",
       "              \n",
       "                      [[-0.2180, -0.0519,  0.0867],\n",
       "                       [-0.2271, -0.0603,  0.0031],\n",
       "                       [-0.1422,  0.1454,  0.1275],\n",
       "                       [ 0.2433,  0.1981,  0.0151],\n",
       "                       [ 0.2541, -0.1612,  0.1946]],\n",
       "              \n",
       "                      [[-0.0032, -0.0670,  0.1032],\n",
       "                       [ 0.1860,  0.1070,  0.0548],\n",
       "                       [ 0.1866, -0.1784,  0.1339],\n",
       "                       [ 0.0991, -0.1859,  0.1546],\n",
       "                       [-0.0271,  0.3086, -0.0484]],\n",
       "              \n",
       "                      [[ 0.0262,  0.1431, -0.0325],\n",
       "                       [-0.0571,  0.0214,  0.1744],\n",
       "                       [ 0.0224, -0.0843,  0.1484],\n",
       "                       [ 0.2340, -0.2222, -0.1784],\n",
       "                       [-0.0203,  0.1291,  0.2551]],\n",
       "              \n",
       "                      [[ 0.0308, -0.0566, -0.1920],\n",
       "                       [ 0.0753,  0.0673,  0.2552],\n",
       "                       [ 0.0294,  0.1894, -0.1210],\n",
       "                       [-0.0168,  0.0895,  0.0795],\n",
       "                       [-0.1214,  0.3175, -0.2418]],\n",
       "              \n",
       "                      [[-0.2461,  0.2116,  0.0585],\n",
       "                       [-0.0844,  0.0659, -0.2433],\n",
       "                       [ 0.2328,  0.0825,  0.0246],\n",
       "                       [-0.1480,  0.1174, -0.1472],\n",
       "                       [ 0.2539, -0.1078, -0.2013]],\n",
       "              \n",
       "                      [[-0.1994,  0.2293,  0.2038],\n",
       "                       [-0.1448,  0.2008,  0.2014],\n",
       "                       [-0.1743,  0.1829,  0.1189],\n",
       "                       [-0.0870,  0.1817,  0.1181],\n",
       "                       [-0.0600, -0.1135,  0.1835]],\n",
       "              \n",
       "                      [[-0.1531,  0.0658,  0.0405],\n",
       "                       [-0.1945,  0.2599,  0.1196],\n",
       "                       [-0.1374, -0.0673,  0.1073],\n",
       "                       [-0.1385,  0.1282, -0.2007],\n",
       "                       [ 0.1583,  0.1042, -0.2197]],\n",
       "              \n",
       "                      [[ 0.1499, -0.0165, -0.0109],\n",
       "                       [ 0.1495,  0.0755,  0.0780],\n",
       "                       [ 0.0765,  0.1034,  0.0684],\n",
       "                       [ 0.0699, -0.0226,  0.0868],\n",
       "                       [ 0.0195, -0.1703, -0.1335]],\n",
       "              \n",
       "                      [[ 0.0403, -0.2007,  0.1743],\n",
       "                       [ 0.0670, -0.0502,  0.2361],\n",
       "                       [ 0.0844,  0.0942, -0.1297],\n",
       "                       [-0.1158,  0.0455, -0.1491],\n",
       "                       [-0.1656,  0.0817,  0.0914]],\n",
       "              \n",
       "                      [[ 0.1032,  0.1872, -0.1205],\n",
       "                       [-0.0880, -0.0838,  0.2563],\n",
       "                       [-0.1019, -0.1778, -0.1981],\n",
       "                       [-0.1267,  0.0031,  0.0560],\n",
       "                       [ 0.0924, -0.2069, -0.0833]],\n",
       "              \n",
       "                      [[ 0.0751, -0.0671, -0.2159],\n",
       "                       [ 0.2077, -0.0900,  0.1487],\n",
       "                       [ 0.2037,  0.1030, -0.0022],\n",
       "                       [-0.2575,  0.1686,  0.1023],\n",
       "                       [-0.1791, -0.1418,  0.0958]],\n",
       "              \n",
       "                      [[-0.1710,  0.1229, -0.0277],\n",
       "                       [ 0.0108, -0.0938,  0.0400],\n",
       "                       [-0.0373,  0.0779, -0.1626],\n",
       "                       [ 0.0284,  0.1610, -0.1307],\n",
       "                       [-0.1404, -0.1948,  0.0963]],\n",
       "              \n",
       "                      [[-0.2650, -0.0232,  0.1463],\n",
       "                       [-0.1327, -0.0077, -0.2452],\n",
       "                       [ 0.2071,  0.0976, -0.2408],\n",
       "                       [ 0.1866,  0.1279,  0.0033],\n",
       "                       [-0.1498,  0.2203,  0.0534]],\n",
       "              \n",
       "                      [[-0.2707, -0.1075,  0.1833],\n",
       "                       [-0.2234, -0.0439, -0.0188],\n",
       "                       [ 0.1293,  0.2041, -0.1493],\n",
       "                       [ 0.1442, -0.0782, -0.2046],\n",
       "                       [-0.1827, -0.0140,  0.1302]],\n",
       "              \n",
       "                      [[-0.0239, -0.0634, -0.1092],\n",
       "                       [ 0.1335, -0.0297, -0.1222],\n",
       "                       [-0.1707, -0.1532,  0.0175],\n",
       "                       [ 0.0711,  0.1144,  0.0752],\n",
       "                       [-0.0494, -0.0687, -0.0399]],\n",
       "              \n",
       "                      [[ 0.1859,  0.0363, -0.0190],\n",
       "                       [ 0.1489,  0.1413, -0.0199],\n",
       "                       [ 0.1493,  0.2465, -0.1183],\n",
       "                       [-0.0486, -0.1435,  0.0845],\n",
       "                       [ 0.0708,  0.1069, -0.1608]],\n",
       "              \n",
       "                      [[ 0.1163, -0.0402, -0.0008],\n",
       "                       [-0.1170,  0.2573, -0.0318],\n",
       "                       [-0.0006,  0.2062, -0.2157],\n",
       "                       [ 0.2505,  0.1595, -0.2115],\n",
       "                       [-0.0991, -0.0743, -0.0118]],\n",
       "              \n",
       "                      [[-0.1124, -0.1994,  0.0645],\n",
       "                       [-0.0805,  0.0101, -0.0673],\n",
       "                       [-0.0672, -0.1510,  0.2399],\n",
       "                       [-0.0391,  0.0312, -0.0126],\n",
       "                       [-0.0852,  0.2549,  0.0637]],\n",
       "              \n",
       "                      [[ 0.0477, -0.1389, -0.2484],\n",
       "                       [ 0.1895, -0.0191,  0.1895],\n",
       "                       [-0.1068, -0.2119, -0.2477],\n",
       "                       [-0.2089,  0.1873, -0.1900],\n",
       "                       [ 0.1310, -0.1317, -0.1208]],\n",
       "              \n",
       "                      [[ 0.2319, -0.2561,  0.0497],\n",
       "                       [-0.1459, -0.0411, -0.0999],\n",
       "                       [-0.0185, -0.1257,  0.0412],\n",
       "                       [-0.2065,  0.2072,  0.1190],\n",
       "                       [-0.0686, -0.0667, -0.1446]],\n",
       "              \n",
       "                      [[ 0.1899,  0.0804,  0.0301],\n",
       "                       [ 0.0647, -0.0642,  0.0763],\n",
       "                       [ 0.0421,  0.0299, -0.0976],\n",
       "                       [-0.1316,  0.2443,  0.2491],\n",
       "                       [-0.2951, -0.0893,  0.1017]],\n",
       "              \n",
       "                      [[-0.2007, -0.1121, -0.0450],\n",
       "                       [ 0.0587,  0.2542,  0.1204],\n",
       "                       [-0.2165, -0.0059,  0.0416],\n",
       "                       [ 0.1783, -0.0759, -0.1474],\n",
       "                       [-0.1837,  0.0976, -0.2566]],\n",
       "              \n",
       "                      [[ 0.2097,  0.1606, -0.1045],\n",
       "                       [-0.0431,  0.2422,  0.1726],\n",
       "                       [ 0.0217,  0.2239, -0.0832],\n",
       "                       [-0.2143,  0.1382, -0.1343],\n",
       "                       [ 0.1117, -0.2256,  0.0111]]], device='cuda:0')),\n",
       "             ('conv1d_1.bias',\n",
       "              tensor([-0.0224, -0.2595, -0.1612,  0.1779,  0.0064, -0.0733, -0.1383, -0.2571,\n",
       "                      -0.0604, -0.1458, -0.1457, -0.1573,  0.2040, -0.2286,  0.0429,  0.1022,\n",
       "                      -0.2370, -0.1148,  0.0916, -0.0565, -0.0864, -0.1664, -0.1401,  0.1377,\n",
       "                      -0.1118,  0.2060,  0.0013,  0.2110, -0.1574,  0.0613, -0.2469, -0.1273],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_ih_l0',\n",
       "              tensor([[ 0.1012, -0.0076,  0.1149,  ..., -0.0420, -0.0010, -0.0705],\n",
       "                      [-0.0183,  0.0008, -0.0219,  ...,  0.0188,  0.0387, -0.0320],\n",
       "                      [-0.0831,  0.1137, -0.0934,  ..., -0.0981,  0.1021, -0.0628],\n",
       "                      ...,\n",
       "                      [-0.0705, -0.0089,  0.0526,  ...,  0.0701,  0.0174,  0.0538],\n",
       "                      [-0.0413,  0.0265,  0.1104,  ..., -0.0004,  0.0802, -0.0503],\n",
       "                      [ 0.0813,  0.0048,  0.0579,  ...,  0.1074, -0.1151,  0.0114]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0694,  0.1129, -0.0255,  ...,  0.1130,  0.0541, -0.0588],\n",
       "                      [ 0.0314,  0.0208, -0.0248,  ...,  0.0541, -0.0673, -0.0979],\n",
       "                      [ 0.0418, -0.0240, -0.1151,  ..., -0.0183,  0.1042, -0.0297],\n",
       "                      ...,\n",
       "                      [ 0.1158, -0.0673, -0.0486,  ...,  0.0824,  0.0288,  0.0141],\n",
       "                      [-0.0083,  0.0486, -0.0471,  ..., -0.1071,  0.1218,  0.1087],\n",
       "                      [-0.0049, -0.1133,  0.0690,  ..., -0.0188,  0.1091, -0.0095]],\n",
       "                     device='cuda:0')),\n",
       "             ('lstm.bias_ih_l0',\n",
       "              tensor([ 5.3868e-02,  9.0596e-02,  7.5457e-02,  6.7158e-02,  5.4537e-02,\n",
       "                       1.0408e-01,  4.7695e-02, -5.7731e-02, -6.1047e-02,  9.5943e-02,\n",
       "                      -3.8652e-02, -6.9828e-02,  1.1057e-01,  5.1594e-02,  1.1027e-01,\n",
       "                       2.7235e-02,  1.0743e-01, -6.8514e-02,  8.2476e-02, -1.1162e-01,\n",
       "                       2.8278e-02, -3.0323e-02,  9.3145e-02,  4.3033e-03,  1.1719e-02,\n",
       "                      -7.2998e-02,  8.7524e-03,  9.3101e-03, -8.6174e-02, -1.0624e-01,\n",
       "                      -3.5125e-02,  7.3363e-04,  2.8138e-02, -6.2531e-02,  5.6682e-02,\n",
       "                       3.1301e-02,  9.1280e-02,  3.2092e-02,  1.4481e-01, -1.1157e-01,\n",
       "                       2.1417e-02,  9.4994e-02,  1.8966e-02,  3.9146e-02, -6.4977e-02,\n",
       "                       7.5756e-02, -7.4760e-03,  1.0402e-01, -1.0627e-01, -7.4747e-02,\n",
       "                       9.2733e-02, -6.0533e-02, -7.0039e-02, -4.5600e-02,  7.7647e-02,\n",
       "                      -2.3613e-02,  8.8121e-02,  9.6149e-02, -5.6405e-03, -1.2054e-01,\n",
       "                      -1.1478e-01, -9.2608e-03, -4.4800e-02,  3.8167e-02,  6.1368e-02,\n",
       "                       6.5860e-02,  4.7880e-02,  7.4530e-02,  9.1856e-02,  6.6993e-02,\n",
       "                       1.0030e-01,  3.2096e-02, -2.2600e-02, -1.2790e-02, -9.6750e-03,\n",
       "                      -5.4499e-02, -1.3675e-02,  1.2005e-01,  1.1131e-02,  1.0839e-01,\n",
       "                       5.2467e-02,  1.1766e-01, -9.4180e-02,  1.1872e-01, -5.3337e-02,\n",
       "                      -1.6175e-02, -9.2171e-02, -2.6488e-02,  9.0732e-02,  1.2864e-01,\n",
       "                       7.1288e-02,  6.9396e-02, -6.5681e-02,  1.4771e-02,  2.9913e-02,\n",
       "                      -1.1206e-01, -4.9773e-02, -3.7829e-02, -6.5436e-02,  3.2413e-02,\n",
       "                      -9.0400e-02,  5.8081e-02,  1.2899e-01, -3.5344e-02,  9.5198e-02,\n",
       "                      -2.2568e-03, -6.2852e-02, -6.0908e-02, -1.2399e-01,  5.6393e-02,\n",
       "                       5.5035e-04, -6.1974e-04,  1.0411e-01, -3.5738e-02, -6.1610e-02,\n",
       "                      -1.1631e-04,  1.1994e-01,  6.0534e-02, -7.3968e-02, -1.1282e-01,\n",
       "                       8.2715e-02, -2.2101e-02,  4.5399e-02,  1.1845e-01, -1.1436e-01,\n",
       "                       1.1513e-01, -3.5054e-02, -1.0084e-01,  4.3179e-02, -1.1956e-02,\n",
       "                       1.0381e-02, -7.4577e-02,  5.9449e-03,  1.3595e-01, -6.0571e-02,\n",
       "                       6.7169e-02,  1.0903e-01,  6.1689e-03,  9.3835e-02, -3.9995e-02,\n",
       "                      -2.0396e-02, -1.3699e-01,  4.8180e-02,  1.2555e-01,  1.4753e-02,\n",
       "                      -6.5713e-02,  6.5179e-02,  4.7227e-02,  9.5856e-02, -1.0118e-02,\n",
       "                      -1.0314e-02, -7.8521e-02,  4.6951e-02, -1.2361e-01,  4.7069e-02,\n",
       "                       2.2740e-02,  1.1836e-01,  1.2173e-01,  5.0971e-02, -1.1734e-01,\n",
       "                       9.7462e-02,  8.8887e-03,  1.0820e-01,  2.3470e-02,  1.0291e-01,\n",
       "                       8.0368e-02, -9.7722e-02, -5.5761e-03, -8.9184e-02,  3.1475e-02,\n",
       "                      -9.3984e-03,  1.1992e-01,  2.7580e-02, -1.0472e-03,  1.4583e-02,\n",
       "                      -9.0684e-02,  9.0394e-03,  2.6079e-02,  1.0191e-02, -7.0554e-03,\n",
       "                       5.8187e-02, -3.1157e-02, -7.6609e-02, -3.7604e-02, -8.5662e-02,\n",
       "                       3.1257e-02, -2.8339e-02, -3.4204e-02, -8.8751e-02, -3.2669e-02,\n",
       "                       2.0643e-02, -5.5806e-03,  4.2990e-02,  9.2517e-02, -2.4020e-02,\n",
       "                       1.2351e-01, -2.3387e-02,  1.2436e-01,  4.7977e-02,  5.9032e-03,\n",
       "                       1.1316e-01,  9.2434e-02, -9.0603e-02,  4.7361e-02, -1.0698e-01,\n",
       "                       2.7483e-02,  2.9221e-02, -8.7389e-02,  2.7014e-02, -5.9737e-02,\n",
       "                       4.0147e-02, -1.2131e-01,  5.9119e-03, -1.3345e-02, -1.0037e-01,\n",
       "                      -1.4939e-02, -6.8794e-03,  6.6602e-02,  7.4688e-02,  4.1141e-02,\n",
       "                       1.1591e-01, -2.4884e-02,  7.3368e-02,  5.2441e-02,  1.0083e-01,\n",
       "                       1.0082e-01, -1.5679e-03, -8.4762e-02, -7.9855e-02,  8.5472e-02,\n",
       "                      -8.8256e-03,  5.6035e-02,  3.6214e-02,  5.1810e-02,  2.2687e-02,\n",
       "                       4.3160e-02, -6.8836e-02,  1.2885e-02,  7.5379e-02,  8.7507e-02,\n",
       "                       6.4343e-02, -5.7886e-03, -1.3210e-02, -1.1188e-01, -6.0587e-02,\n",
       "                      -3.6494e-02,  9.3816e-02,  7.4361e-02,  2.3228e-02,  5.9486e-02,\n",
       "                      -6.0791e-02,  7.1012e-02,  5.0216e-02, -1.2451e-01,  8.5068e-02,\n",
       "                      -6.6489e-02], device='cuda:0')),\n",
       "             ('lstm.bias_hh_l0',\n",
       "              tensor([ 1.0534e-01,  7.0975e-02, -8.6956e-02,  1.4072e-02, -3.4889e-02,\n",
       "                       1.2161e-01,  9.6539e-02, -3.3042e-02,  7.4862e-02,  8.1215e-02,\n",
       "                      -7.3880e-02, -9.8221e-02,  1.6558e-02,  1.3184e-01,  1.0898e-01,\n",
       "                      -3.6811e-02, -1.0262e-01,  6.8183e-02,  7.9405e-02,  1.0390e-01,\n",
       "                      -7.4176e-02, -5.6861e-02,  2.6727e-02,  6.0532e-03,  2.2951e-02,\n",
       "                       5.9389e-02, -6.1124e-02, -8.7127e-02, -3.4967e-02, -1.1663e-01,\n",
       "                      -1.0267e-02,  5.2385e-02,  3.8550e-02,  1.0958e-01, -3.3255e-02,\n",
       "                       2.5578e-02,  7.1072e-02,  8.4059e-02, -8.5146e-02,  9.0620e-02,\n",
       "                       8.4210e-02,  1.3020e-02, -2.2150e-02,  4.9146e-02, -1.2051e-01,\n",
       "                      -7.9710e-02, -1.1471e-02,  3.8699e-02, -3.5011e-03, -1.2236e-01,\n",
       "                      -6.0934e-02,  1.0715e-01,  4.2925e-02,  3.8306e-02, -1.2706e-01,\n",
       "                       6.6683e-02, -2.1779e-02,  8.2384e-02,  1.1420e-01, -1.0465e-01,\n",
       "                       2.9048e-02, -1.1694e-02,  1.1660e-01,  5.3914e-02, -2.4081e-02,\n",
       "                      -2.9564e-04, -1.6880e-02,  1.1944e-01,  1.3450e-01,  4.1881e-02,\n",
       "                       1.2108e-02, -9.1452e-02, -4.1166e-02,  4.6692e-02, -8.3824e-02,\n",
       "                      -6.3718e-02,  5.9096e-03, -8.6247e-03,  3.5950e-02, -1.2684e-01,\n",
       "                       5.3854e-02, -1.0398e-01,  7.9979e-02, -4.4006e-02,  1.2324e-01,\n",
       "                       6.3216e-02,  2.9186e-02,  2.8370e-02, -2.9585e-02, -1.5353e-02,\n",
       "                      -7.8851e-02,  5.5064e-02,  6.2628e-02,  3.4558e-02, -1.0110e-01,\n",
       "                       1.2288e-02, -4.6719e-02, -1.0811e-01,  2.8157e-02, -6.0746e-02,\n",
       "                      -8.1564e-03, -2.2972e-02,  1.0401e-01,  7.1326e-02,  6.4959e-02,\n",
       "                       4.7720e-03, -6.8429e-02,  4.8275e-02, -3.4779e-02,  1.3051e-01,\n",
       "                      -5.6970e-02,  1.0343e-01,  8.0693e-02, -7.2510e-02, -7.1157e-02,\n",
       "                      -6.8937e-03,  1.2100e-02, -8.7662e-02, -6.0836e-02,  1.0284e-01,\n",
       "                      -4.6135e-02,  8.9139e-02, -9.0767e-03, -7.3280e-03, -1.2480e-01,\n",
       "                       1.0471e-01,  6.8294e-03, -8.7970e-03, -2.3665e-02,  2.7555e-02,\n",
       "                       1.2276e-01,  1.0634e-01, -1.0422e-01,  5.3755e-02,  1.0974e-01,\n",
       "                      -6.7378e-02, -2.3396e-02,  1.3014e-01, -3.5873e-03,  1.0021e-01,\n",
       "                       5.1254e-02, -8.2472e-02, -6.7492e-02, -5.6448e-02,  1.0316e-01,\n",
       "                       6.7756e-03,  9.6986e-02,  9.7219e-02, -7.4133e-02, -5.7012e-02,\n",
       "                       1.2095e-01,  7.3609e-02,  1.0963e-01, -7.4958e-03,  8.5779e-02,\n",
       "                       1.1340e-01,  3.6162e-02,  3.3031e-02, -1.1147e-01,  3.0074e-02,\n",
       "                      -1.2115e-01, -5.4943e-03, -3.6258e-02, -6.8248e-02,  2.5077e-02,\n",
       "                       3.1416e-02,  7.9620e-02,  2.3637e-02, -1.2039e-01, -2.2486e-02,\n",
       "                       1.2990e-02,  1.0926e-01,  1.0608e-01, -3.4510e-02,  1.1526e-01,\n",
       "                       5.9439e-03,  1.2274e-01,  8.6152e-02, -4.9044e-02, -6.9870e-02,\n",
       "                       1.1453e-01,  7.0160e-02, -1.4208e-02,  1.0316e-01, -3.0262e-03,\n",
       "                      -5.3040e-02,  7.1365e-02,  7.0940e-02,  1.0311e-01, -5.0396e-02,\n",
       "                      -3.9563e-02, -6.1612e-02, -1.1448e-03, -3.5247e-02,  1.0525e-01,\n",
       "                      -4.0147e-02,  7.2371e-02, -7.0084e-02,  9.0259e-02, -2.8638e-02,\n",
       "                       1.0338e-01,  9.9826e-03,  1.1412e-01, -1.8818e-03,  4.8621e-02,\n",
       "                      -4.3557e-02,  1.1825e-01, -1.2904e-02,  1.0969e-01, -8.3570e-02,\n",
       "                      -9.6824e-03, -2.5965e-02, -4.5075e-02,  1.0403e-01, -9.7535e-02,\n",
       "                      -4.5948e-02,  5.7712e-02, -2.7687e-02,  9.0597e-02, -3.4000e-02,\n",
       "                       7.3007e-02, -1.0376e-01, -5.6308e-02, -1.1555e-01,  1.0537e-01,\n",
       "                       1.1206e-01, -8.8401e-02,  1.0981e-01,  1.2594e-01,  3.6722e-03,\n",
       "                       1.3913e-01,  4.7704e-03, -1.1523e-01,  5.3484e-03, -3.6863e-02,\n",
       "                      -2.7326e-02,  9.4590e-02,  6.4114e-03,  9.1786e-02, -5.8381e-02,\n",
       "                       1.3025e-01, -2.5510e-02,  1.2606e-01, -7.6949e-02,  7.9088e-02,\n",
       "                      -1.1175e-01, -5.9332e-02, -5.0509e-05,  6.0793e-03,  7.3638e-02,\n",
       "                       7.4423e-02,  2.8812e-02, -4.5473e-02, -1.1807e-01, -8.5863e-02,\n",
       "                       1.1237e-01], device='cuda:0')),\n",
       "             ('fc.0.weight',\n",
       "              tensor([[-0.0026, -0.0929,  0.0709,  ...,  0.0746,  0.0626, -0.1133],\n",
       "                      [ 0.0402,  0.0021, -0.0284,  ...,  0.0353,  0.1182,  0.0420],\n",
       "                      [ 0.1023, -0.1129, -0.0899,  ...,  0.0736, -0.1074, -0.0096],\n",
       "                      ...,\n",
       "                      [-0.0971,  0.1236,  0.0408,  ...,  0.0690, -0.0804, -0.0149],\n",
       "                      [ 0.1103,  0.0009, -0.0485,  ..., -0.0083,  0.1058, -0.0683],\n",
       "                      [ 0.0519, -0.1214,  0.0840,  ...,  0.0472, -0.0845,  0.0632]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.0.bias',\n",
       "              tensor([ 0.0490, -0.0214, -0.0070, -0.1090,  0.0980, -0.0197, -0.0635, -0.0665,\n",
       "                      -0.0707,  0.0410,  0.0477, -0.0834,  0.1084,  0.0040,  0.0270, -0.1171,\n",
       "                       0.1065,  0.0402, -0.0308, -0.0068,  0.0569, -0.0587,  0.0410,  0.0209,\n",
       "                      -0.0106,  0.1131, -0.0534, -0.0367,  0.1229,  0.0226,  0.0963,  0.0488],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.3.weight',\n",
       "              tensor([[ 1.3267e-02, -4.1027e-02,  2.0290e-02, -1.5254e-01,  1.5220e-02,\n",
       "                        9.7341e-02, -3.3920e-02, -1.2324e-01, -9.5095e-02, -7.7885e-02,\n",
       "                        1.7334e-01, -1.0609e-01,  8.1518e-02, -1.0564e-01,  5.9693e-02,\n",
       "                       -4.8858e-02,  1.0531e-01, -1.6129e-01,  2.4320e-02, -4.1153e-02,\n",
       "                        1.1834e-01, -2.4682e-02,  3.9865e-02,  1.3992e-01, -3.6418e-02,\n",
       "                       -7.2662e-02, -1.3751e-01,  3.6478e-02,  5.9047e-02, -3.6013e-02,\n",
       "                        8.9237e-02,  9.5458e-02],\n",
       "                      [ 5.7885e-02, -1.3590e-01,  1.5708e-01, -1.5295e-01, -8.5120e-02,\n",
       "                       -1.4504e-02, -6.2064e-02, -1.2243e-01, -5.4716e-02,  1.2523e-01,\n",
       "                        3.4104e-02, -4.0466e-02,  1.6114e-01, -1.6317e-01,  1.6649e-01,\n",
       "                       -3.2916e-03,  5.5065e-02,  5.8888e-02, -3.5699e-02,  3.4608e-02,\n",
       "                       -1.2899e-01, -8.7263e-02,  1.3993e-01,  1.4170e-01, -5.5265e-02,\n",
       "                        2.4237e-02,  1.5321e-01,  8.9420e-03,  1.7785e-01,  6.1634e-02,\n",
       "                       -4.6560e-02,  1.8421e-01],\n",
       "                      [ 2.8476e-02, -5.2268e-02, -1.0448e-01,  4.7380e-02,  1.1535e-01,\n",
       "                       -6.8752e-03,  1.5030e-01, -1.0376e-01,  1.0863e-01, -1.3869e-01,\n",
       "                        9.7313e-02, -4.7884e-02,  5.6309e-02, -1.6921e-01, -1.2659e-01,\n",
       "                       -2.7417e-02,  2.1222e-02, -2.3869e-02,  6.2696e-02,  7.3306e-02,\n",
       "                        1.0500e-01,  2.0872e-02,  4.1472e-02,  6.1000e-02, -1.5936e-01,\n",
       "                        1.1541e-01,  1.1606e-01, -1.1582e-01,  7.7772e-02,  9.8581e-02,\n",
       "                        1.2385e-01,  6.6157e-02],\n",
       "                      [ 1.3756e-03,  1.3137e-01,  1.5585e-02, -9.1603e-02,  1.2530e-01,\n",
       "                       -4.6940e-02, -7.7882e-02,  1.4786e-01, -8.0275e-02, -6.8780e-02,\n",
       "                        9.3168e-02,  1.7735e-02, -2.5174e-02, -2.1742e-02,  7.6141e-02,\n",
       "                        1.3860e-01, -8.7285e-02, -6.3779e-02, -3.9837e-03,  1.2305e-02,\n",
       "                       -7.8221e-02,  1.3400e-04,  3.8906e-02, -1.8750e-02,  6.5436e-02,\n",
       "                       -2.0882e-02,  1.9633e-02,  1.1038e-02, -1.1556e-01,  1.1987e-01,\n",
       "                        1.2156e-01, -1.4899e-02],\n",
       "                      [-4.2474e-02, -5.6838e-02,  1.1879e-01,  1.7736e-01,  1.3409e-01,\n",
       "                       -8.9281e-02, -1.7876e-01,  7.2037e-02, -2.1290e-01,  8.1272e-02,\n",
       "                        6.9193e-02,  6.7861e-02, -1.0546e-01, -1.3722e-01,  1.3502e-01,\n",
       "                        3.9062e-02, -8.6534e-02, -1.6021e-01, -6.1842e-02, -9.3568e-02,\n",
       "                        4.1630e-02, -6.5397e-02,  7.1438e-02,  1.0773e-01,  7.1582e-02,\n",
       "                        3.8757e-04,  1.3296e-01, -1.0171e-01,  4.7832e-03, -8.4860e-03,\n",
       "                        7.9739e-03, -1.2441e-01],\n",
       "                      [ 5.3755e-02,  1.7725e-01,  2.0548e-01,  9.0049e-02,  1.7412e-01,\n",
       "                       -1.5097e-01,  1.6962e-01, -1.7297e-01,  1.9510e-01, -2.8936e-02,\n",
       "                        1.8876e-01, -8.2234e-02, -1.0657e-01,  1.5736e-01,  1.2538e-01,\n",
       "                       -4.8406e-02,  1.5669e-01,  1.3677e-01,  4.5409e-02,  1.1170e-01,\n",
       "                       -9.0247e-02,  1.0252e-01,  1.1343e-03,  1.2606e-01, -9.6705e-02,\n",
       "                        4.7974e-02, -1.0132e-01,  1.5998e-01,  2.1708e-01,  7.6941e-02,\n",
       "                        1.5590e-01,  4.3932e-02],\n",
       "                      [ 1.7017e-01, -1.2584e-01,  1.7492e-01, -1.2931e-01,  2.0506e-01,\n",
       "                       -7.4066e-02,  9.1625e-02, -1.3574e-01,  1.0380e-01,  1.2739e-01,\n",
       "                        9.3200e-02, -7.8180e-02,  1.7408e-01,  2.1579e-02,  7.7681e-02,\n",
       "                        9.3189e-02,  1.7879e-01, -1.2155e-01,  3.7071e-02, -3.9142e-02,\n",
       "                       -9.6928e-02,  3.0797e-02,  1.7672e-02,  1.3124e-01,  8.7362e-02,\n",
       "                        6.8553e-02, -1.2354e-01,  8.4554e-03,  1.7616e-01,  1.2119e-02,\n",
       "                        4.9678e-02,  6.8990e-02],\n",
       "                      [-7.2089e-02, -2.3962e-02,  4.7433e-02,  2.4120e-02,  9.2279e-02,\n",
       "                       -1.1070e-01,  2.9747e-03,  1.7340e-01, -4.8463e-02, -4.0017e-03,\n",
       "                        2.6030e-02,  7.7716e-02, -4.8306e-02, -4.1837e-02,  1.5107e-01,\n",
       "                       -4.1385e-02,  1.4124e-01, -3.2710e-02,  6.9423e-02, -4.3295e-02,\n",
       "                       -1.0593e-01, -5.0331e-02,  6.2684e-02, -7.8222e-02, -1.2670e-01,\n",
       "                       -2.4722e-02, -5.8758e-02, -2.8809e-02,  8.0269e-02, -6.2556e-02,\n",
       "                       -1.2908e-01,  6.5170e-02],\n",
       "                      [ 1.8610e-02,  1.1893e-01, -4.4313e-02,  9.2855e-02, -8.8978e-02,\n",
       "                       -5.4468e-02,  1.3678e-01, -9.9558e-02,  1.4302e-01, -8.2138e-03,\n",
       "                        1.9432e-01, -1.6393e-01,  2.7786e-02,  6.5289e-02, -2.6990e-02,\n",
       "                       -6.1869e-03,  1.0062e-01, -8.3233e-02, -8.1835e-02,  2.1677e-01,\n",
       "                        1.7535e-01, -8.5206e-02,  1.8475e-01,  2.0447e-01,  8.9727e-02,\n",
       "                       -7.1595e-02, -4.8734e-02, -1.3435e-01, -6.0914e-02,  5.2551e-02,\n",
       "                       -1.2519e-01, -6.4044e-02],\n",
       "                      [ 1.6780e-01, -1.4669e-01, -1.5817e-01,  6.8968e-02,  4.8376e-02,\n",
       "                       -1.0965e-01, -1.2491e-01,  3.9337e-02,  1.7307e-01, -1.0330e-02,\n",
       "                       -9.7496e-02, -1.0649e-01, -8.4134e-02, -1.0502e-01, -5.5078e-02,\n",
       "                       -1.3828e-01, -4.4231e-02,  1.4985e-01, -5.1117e-02, -1.3011e-02,\n",
       "                       -1.6529e-01,  3.8008e-02, -1.1314e-01,  6.8937e-02,  1.6626e-01,\n",
       "                       -1.0556e-01, -1.3859e-01, -4.7356e-03, -6.5186e-02,  4.4035e-02,\n",
       "                        3.8218e-02, -1.0273e-02],\n",
       "                      [ 7.4229e-04, -7.8047e-02,  3.8154e-02, -1.6458e-02,  7.6453e-02,\n",
       "                       -8.2847e-02, -9.3882e-02, -1.3791e-01, -1.4373e-02, -1.4674e-01,\n",
       "                        1.6104e-01, -1.7575e-01,  1.8354e-01,  1.2577e-02,  1.3906e-01,\n",
       "                        1.7823e-01,  9.0001e-02,  1.0733e-01,  4.2783e-02,  1.7792e-01,\n",
       "                       -3.0159e-02, -1.4747e-01,  1.3174e-01,  1.1720e-01, -9.1831e-02,\n",
       "                       -2.9235e-02,  1.2967e-01, -3.6514e-02, -6.3999e-03,  1.6481e-01,\n",
       "                        1.7410e-01, -1.1958e-01],\n",
       "                      [ 1.3477e-01,  1.0339e-02,  1.7624e-01,  2.3737e-02, -9.0304e-02,\n",
       "                        9.5699e-02,  1.0695e-02,  1.5538e-01, -1.7000e-02, -7.4869e-02,\n",
       "                        1.6704e-01, -8.0697e-02, -9.2319e-02, -1.5381e-01, -3.4847e-02,\n",
       "                        1.1939e-03, -1.1959e-02, -8.9830e-02,  1.6182e-01,  1.5025e-01,\n",
       "                       -1.3139e-01, -6.3649e-03,  2.4742e-02,  1.8180e-01, -2.4928e-02,\n",
       "                        1.6632e-01, -1.7145e-01,  4.7491e-02,  2.6405e-02, -1.6831e-01,\n",
       "                       -3.5530e-02,  5.8386e-02],\n",
       "                      [ 1.9224e-01, -1.6283e-01,  7.0720e-02,  1.1004e-01,  5.8734e-02,\n",
       "                       -5.9611e-02, -9.0068e-02,  1.3290e-01,  2.6383e-03,  1.1650e-01,\n",
       "                        7.9724e-02, -1.1428e-01, -9.3515e-03,  6.5447e-02,  1.5810e-01,\n",
       "                       -1.9436e-02, -1.1577e-01,  2.8260e-02,  2.2829e-01,  8.9295e-02,\n",
       "                       -3.2916e-02, -9.5109e-02,  1.9870e-01,  2.2534e-01,  1.4384e-01,\n",
       "                        1.4865e-01,  2.1840e-02, -9.8762e-03,  7.6893e-03,  6.1666e-02,\n",
       "                       -6.5081e-02, -4.0863e-02],\n",
       "                      [-1.0333e-01,  1.5403e-01,  1.5981e-01,  1.6611e-01,  1.9264e-01,\n",
       "                        2.5987e-02,  7.9122e-02, -2.4044e-03,  6.3824e-03,  2.0335e-02,\n",
       "                       -8.5525e-02,  1.6102e-02,  1.6484e-01, -2.1904e-02, -5.5308e-03,\n",
       "                        4.9925e-02,  5.8551e-04, -8.2584e-02,  7.5109e-02,  1.1281e-01,\n",
       "                       -1.5789e-01,  6.7722e-02,  1.6860e-01, -1.0629e-01, -1.7413e-01,\n",
       "                       -3.1797e-02, -6.6346e-03, -2.0943e-02,  1.4123e-01, -2.0918e-02,\n",
       "                       -1.2184e-01,  2.1269e-01],\n",
       "                      [-8.0289e-02,  1.2564e-02,  1.5818e-02, -1.2477e-01,  1.3348e-01,\n",
       "                       -1.4122e-01, -1.1040e-01, -1.6795e-01, -1.6007e-01,  1.2058e-01,\n",
       "                       -2.9151e-02,  1.6410e-01,  1.4222e-01, -1.5211e-01, -1.4019e-01,\n",
       "                       -9.5274e-02, -9.6383e-02,  8.5932e-02,  1.1128e-01,  3.0997e-02,\n",
       "                       -1.4634e-01,  1.7762e-01,  1.2425e-01, -1.1881e-01, -1.5471e-01,\n",
       "                        1.2209e-02,  8.7923e-02,  4.2843e-02, -6.9708e-02,  2.5469e-02,\n",
       "                       -7.2592e-02, -3.8960e-02],\n",
       "                      [ 6.7471e-02,  1.4078e-01, -1.1320e-01,  1.4603e-02,  5.5684e-03,\n",
       "                       -7.7237e-02, -3.6169e-03, -9.5031e-02, -4.4844e-02, -5.1417e-02,\n",
       "                        2.7979e-02, -1.1381e-01, -1.0117e-01, -4.7739e-02, -6.8376e-02,\n",
       "                        1.0620e-01, -3.2083e-02, -1.2146e-01,  9.8946e-02,  9.2638e-02,\n",
       "                        1.2435e-01,  1.3009e-01,  2.7403e-02,  1.2305e-01, -1.6868e-01,\n",
       "                        1.3766e-01,  4.9568e-02, -1.1422e-01,  1.5774e-02,  1.0360e-01,\n",
       "                       -1.2177e-01,  1.1539e-01],\n",
       "                      [ 4.2070e-02,  1.4108e-01, -3.7217e-02,  1.0763e-01,  5.0780e-02,\n",
       "                       -1.6583e-01, -1.1454e-01,  1.0279e-01, -1.0642e-01,  6.5039e-02,\n",
       "                        3.1713e-02,  1.5347e-02, -5.8809e-03,  4.8033e-02,  8.0259e-02,\n",
       "                        1.1827e-01, -1.1164e-02, -4.4634e-02, -2.6801e-02, -2.4364e-03,\n",
       "                       -8.8159e-02,  3.7185e-02,  4.9381e-03,  5.5628e-02, -8.5519e-02,\n",
       "                       -6.7735e-02,  4.4126e-02, -7.8350e-02,  2.0556e-02, -5.6831e-02,\n",
       "                       -1.6172e-01,  4.8204e-02],\n",
       "                      [ 8.4069e-02,  8.8097e-02,  1.0010e-01, -1.4073e-01,  1.5620e-01,\n",
       "                        2.8544e-02,  1.7805e-01,  2.2210e-02, -9.1660e-02, -1.7717e-01,\n",
       "                       -7.4822e-02, -1.1280e-01,  1.7855e-01,  7.5439e-03, -4.3816e-02,\n",
       "                        1.5288e-01,  3.0718e-02,  4.7301e-02, -1.9485e-03,  1.2220e-01,\n",
       "                       -1.2107e-01,  8.6874e-02, -9.1502e-02,  3.9180e-02, -1.0437e-01,\n",
       "                        6.8186e-02,  1.4684e-01, -2.5766e-02,  9.3812e-02,  4.8632e-02,\n",
       "                       -3.2110e-02,  1.0535e-01],\n",
       "                      [-7.2855e-02, -6.2901e-02,  2.4264e-02,  7.0992e-02, -1.1705e-01,\n",
       "                       -1.5852e-01,  6.8623e-02, -7.5835e-02, -5.6544e-02, -1.5668e-01,\n",
       "                        4.8819e-02, -7.1573e-02,  1.3291e-01,  1.6276e-01,  1.4865e-01,\n",
       "                       -9.8664e-02, -7.8555e-02, -7.7962e-02, -1.1504e-01,  9.0128e-02,\n",
       "                        6.8936e-02,  9.8502e-02, -1.5174e-02, -4.4148e-02,  1.5451e-01,\n",
       "                       -1.2037e-01, -7.2132e-02, -1.5950e-01,  6.4138e-02,  1.6099e-01,\n",
       "                        3.1631e-02, -9.7850e-02],\n",
       "                      [ 1.7531e-01,  1.3875e-01,  8.8336e-03,  1.8580e-01, -4.2391e-02,\n",
       "                       -6.0454e-02,  6.4238e-02,  2.9811e-02,  1.6962e-01, -1.0602e-01,\n",
       "                       -8.2120e-02, -1.5858e-01,  3.3465e-02,  1.5762e-01,  5.8153e-02,\n",
       "                        7.8824e-02, -6.5347e-03,  1.1312e-01,  8.8340e-02,  1.2434e-01,\n",
       "                       -7.0667e-02, -8.0278e-02,  6.6037e-02,  1.7193e-01,  7.9014e-02,\n",
       "                        1.3639e-01, -5.2061e-02,  1.5986e-01,  1.8457e-01, -1.4367e-01,\n",
       "                       -9.1204e-02, -1.7569e-02],\n",
       "                      [-3.5139e-02,  8.1407e-02, -9.9302e-02,  8.2414e-02, -1.4198e-01,\n",
       "                       -1.3355e-01, -9.7098e-02,  1.5092e-01,  1.2431e-01, -7.3400e-02,\n",
       "                        4.8698e-02, -9.9631e-02, -7.0838e-02, -3.9974e-02,  4.8190e-02,\n",
       "                        1.1335e-01, -1.0137e-01, -1.9515e-02, -6.2346e-02,  7.3508e-02,\n",
       "                       -1.7073e-02,  3.5429e-02,  3.9207e-02, -5.3959e-02, -1.0496e-01,\n",
       "                        7.6099e-02,  2.6666e-02,  2.5391e-02,  4.6472e-02,  1.2508e-01,\n",
       "                        1.5306e-01,  1.1895e-02]], device='cuda:0')),\n",
       "             ('fc.3.bias',\n",
       "              tensor([ 0.0224, -0.0695,  0.0163,  0.1054,  0.1080, -0.1011, -0.1073,  0.0421,\n",
       "                      -0.0225, -0.1755, -0.0275, -0.0397, -0.0983, -0.0825, -0.1577,  0.0434,\n",
       "                       0.0827,  0.0009,  0.1063, -0.0715,  0.1057], device='cuda:0'))])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            \n",
    "            # 모델 출력인 output을 CPU로 이동하고 numpy 배열로 변환\n",
    "            output = output.cpu().numpy()\n",
    "            \n",
    "            predictions.extend(output)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4ff439c9284576a0d63f15443da7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(15890, 21)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = inference(infer_model, test_loader, device)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(PATH + '/sample_submission.csv')\n",
    "submit.iloc[:, 1:] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_SUBMIT = os.getcwd() + '/data/cnn_lstm_test.csv'\n",
    "submit.to_csv(SAVE_SUBMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>2023-04-05</th>\n",
       "      <th>2023-04-06</th>\n",
       "      <th>2023-04-07</th>\n",
       "      <th>2023-04-08</th>\n",
       "      <th>2023-04-09</th>\n",
       "      <th>2023-04-10</th>\n",
       "      <th>2023-04-11</th>\n",
       "      <th>2023-04-12</th>\n",
       "      <th>2023-04-13</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-04-16</th>\n",
       "      <th>2023-04-17</th>\n",
       "      <th>2023-04-18</th>\n",
       "      <th>2023-04-19</th>\n",
       "      <th>2023-04-20</th>\n",
       "      <th>2023-04-21</th>\n",
       "      <th>2023-04-22</th>\n",
       "      <th>2023-04-23</th>\n",
       "      <th>2023-04-24</th>\n",
       "      <th>2023-04-25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.074958</td>\n",
       "      <td>0.021919</td>\n",
       "      <td>0.058938</td>\n",
       "      <td>0.063323</td>\n",
       "      <td>0.077961</td>\n",
       "      <td>0.033219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072217</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>0.046388</td>\n",
       "      <td>0.057272</td>\n",
       "      <td>0.031109</td>\n",
       "      <td>0.083085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.067382</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.062163</td>\n",
       "      <td>0.064648</td>\n",
       "      <td>0.083207</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059975</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071236</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.051398</td>\n",
       "      <td>0.052051</td>\n",
       "      <td>0.032542</td>\n",
       "      <td>0.082119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.067413</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.062136</td>\n",
       "      <td>0.065073</td>\n",
       "      <td>0.083972</td>\n",
       "      <td>0.031766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060243</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070908</td>\n",
       "      <td>0.083447</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.052545</td>\n",
       "      <td>0.031610</td>\n",
       "      <td>0.081690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.067411</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>0.062136</td>\n",
       "      <td>0.065076</td>\n",
       "      <td>0.083977</td>\n",
       "      <td>0.031768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060244</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070906</td>\n",
       "      <td>0.083448</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.081694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.058988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065890</td>\n",
       "      <td>0.063309</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053872</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>0.062819</td>\n",
       "      <td>0.052554</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.065357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15885</th>\n",
       "      <td>15885</td>\n",
       "      <td>0.084488</td>\n",
       "      <td>0.056618</td>\n",
       "      <td>0.084525</td>\n",
       "      <td>0.094908</td>\n",
       "      <td>0.097554</td>\n",
       "      <td>0.057195</td>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.087004</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067580</td>\n",
       "      <td>0.031295</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078733</td>\n",
       "      <td>0.086771</td>\n",
       "      <td>0.074674</td>\n",
       "      <td>0.092337</td>\n",
       "      <td>0.042366</td>\n",
       "      <td>0.087180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15886</th>\n",
       "      <td>15886</td>\n",
       "      <td>0.082372</td>\n",
       "      <td>0.054405</td>\n",
       "      <td>0.083648</td>\n",
       "      <td>0.093777</td>\n",
       "      <td>0.096568</td>\n",
       "      <td>0.052019</td>\n",
       "      <td>0.049801</td>\n",
       "      <td>0.084127</td>\n",
       "      <td>0.048317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066425</td>\n",
       "      <td>0.029713</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.085397</td>\n",
       "      <td>0.074005</td>\n",
       "      <td>0.090643</td>\n",
       "      <td>0.041790</td>\n",
       "      <td>0.087124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15887</th>\n",
       "      <td>15887</td>\n",
       "      <td>0.082361</td>\n",
       "      <td>0.054393</td>\n",
       "      <td>0.083643</td>\n",
       "      <td>0.093771</td>\n",
       "      <td>0.096563</td>\n",
       "      <td>0.051995</td>\n",
       "      <td>0.049787</td>\n",
       "      <td>0.084114</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066420</td>\n",
       "      <td>0.029706</td>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.085390</td>\n",
       "      <td>0.074001</td>\n",
       "      <td>0.090634</td>\n",
       "      <td>0.041787</td>\n",
       "      <td>0.087123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15888</th>\n",
       "      <td>15888</td>\n",
       "      <td>0.081895</td>\n",
       "      <td>0.054128</td>\n",
       "      <td>0.083854</td>\n",
       "      <td>0.093492</td>\n",
       "      <td>0.096244</td>\n",
       "      <td>0.051313</td>\n",
       "      <td>0.049531</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.048328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066638</td>\n",
       "      <td>0.030191</td>\n",
       "      <td>0.023226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079321</td>\n",
       "      <td>0.085127</td>\n",
       "      <td>0.074197</td>\n",
       "      <td>0.089995</td>\n",
       "      <td>0.042427</td>\n",
       "      <td>0.087072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15889</th>\n",
       "      <td>15889</td>\n",
       "      <td>0.075861</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>0.080269</td>\n",
       "      <td>0.088445</td>\n",
       "      <td>0.089986</td>\n",
       "      <td>0.048691</td>\n",
       "      <td>0.045743</td>\n",
       "      <td>0.081646</td>\n",
       "      <td>0.041110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064467</td>\n",
       "      <td>0.019146</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073831</td>\n",
       "      <td>0.076992</td>\n",
       "      <td>0.068375</td>\n",
       "      <td>0.083424</td>\n",
       "      <td>0.036505</td>\n",
       "      <td>0.083354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15890 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  2023-04-05  2023-04-06  2023-04-07  2023-04-08  2023-04-09  \\\n",
       "0          0    0.074958    0.021919    0.058938    0.063323    0.077961   \n",
       "1          1    0.067382    0.019339    0.062163    0.064648    0.083207   \n",
       "2          2    0.067413    0.018728    0.062136    0.065073    0.083972   \n",
       "3          3    0.067411    0.018729    0.062136    0.065076    0.083977   \n",
       "4          4    0.058988    0.000000    0.065890    0.063309    0.063863   \n",
       "...      ...         ...         ...         ...         ...         ...   \n",
       "15885  15885    0.084488    0.056618    0.084525    0.094908    0.097554   \n",
       "15886  15886    0.082372    0.054405    0.083648    0.093777    0.096568   \n",
       "15887  15887    0.082361    0.054393    0.083643    0.093771    0.096563   \n",
       "15888  15888    0.081895    0.054128    0.083854    0.093492    0.096244   \n",
       "15889  15889    0.075861    0.042119    0.080269    0.088445    0.089986   \n",
       "\n",
       "       2023-04-10  2023-04-11  2023-04-12  2023-04-13  ...  2023-04-16  \\\n",
       "0        0.033219    0.000000    0.062225    0.004978  ...    0.028688   \n",
       "1        0.032057    0.000000    0.059975    0.001315  ...    0.023031   \n",
       "2        0.031766    0.000000    0.060243    0.001488  ...    0.022108   \n",
       "3        0.031768    0.000000    0.060244    0.001481  ...    0.022104   \n",
       "4        0.000000    0.000317    0.045002    0.000000  ...    0.008858   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "15885    0.057195    0.052711    0.087004    0.050217  ...    0.067580   \n",
       "15886    0.052019    0.049801    0.084127    0.048317  ...    0.066425   \n",
       "15887    0.051995    0.049787    0.084114    0.048307  ...    0.066420   \n",
       "15888    0.051313    0.049531    0.083500    0.048328  ...    0.066638   \n",
       "15889    0.048691    0.045743    0.081646    0.041110  ...    0.064467   \n",
       "\n",
       "       2023-04-17  2023-04-18  2023-04-19  2023-04-20  2023-04-21  2023-04-22  \\\n",
       "0        0.000000    0.000000         0.0    0.072217    0.085864    0.046388   \n",
       "1        0.000000    0.000000         0.0    0.071236    0.083069    0.051398   \n",
       "2        0.000000    0.000000         0.0    0.070908    0.083447    0.051409   \n",
       "3        0.000000    0.000000         0.0    0.070906    0.083448    0.051409   \n",
       "4        0.000000    0.000000         0.0    0.053872    0.066913    0.062819   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "15885    0.031295    0.026144         0.0    0.078733    0.086771    0.074674   \n",
       "15886    0.029713    0.023569         0.0    0.078804    0.085397    0.074005   \n",
       "15887    0.029706    0.023555         0.0    0.078804    0.085390    0.074001   \n",
       "15888    0.030191    0.023226         0.0    0.079321    0.085127    0.074197   \n",
       "15889    0.019146    0.013924         0.0    0.073831    0.076992    0.068375   \n",
       "\n",
       "       2023-04-23  2023-04-24  2023-04-25  \n",
       "0        0.057272    0.031109    0.083085  \n",
       "1        0.052051    0.032542    0.082119  \n",
       "2        0.052545    0.031610    0.081690  \n",
       "3        0.052544    0.031609    0.081694  \n",
       "4        0.052554    0.011568    0.065357  \n",
       "...           ...         ...         ...  \n",
       "15885    0.092337    0.042366    0.087180  \n",
       "15886    0.090643    0.041790    0.087124  \n",
       "15887    0.090634    0.041787    0.087123  \n",
       "15888    0.089995    0.042427    0.087072  \n",
       "15889    0.083424    0.036505    0.083354  \n",
       "\n",
       "[15890 rows x 22 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
